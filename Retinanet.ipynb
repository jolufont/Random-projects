!unzip Judol_Detection_v2.v8i.retinanet.zip

"kalau space ada 3 itu berarti cell berikutnya

import torch, pandas as pd, os
from torch.utils.data import Dataset
from torchvision import transforms
from PIL import Image

transform = transforms.Compose([
    transforms.ToTensor(),
])

class ObjectDetectionDataset(Dataset):
    def __init__(self, dataframe, img_dir, class_dict, transform=None):
        self.dataframe = dataframe
        self.img_dir = img_dir
        self.class_dict = class_dict
        self.transform = transform

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        row = self.dataframe.iloc[idx]

        image_name = row['image_name']
        img_path = os.path.join(self.img_dir, image_name)

        # Load image
        image = Image.open(img_path).convert("RGB")

        # Get bounding box and label
        xmin, ymin, xmax, ymax = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])
        label_name = row['label']

        # Map label name to their class ID
        label_id = int(self.class_dict.get(label_name, -1))  # Default to -1 if not found

        # Prepare bounding boxes and labels
        boxes = torch.tensor([[xmin, ymin, xmax, ymax]], dtype=torch.float32)
        labels = torch.tensor([label_id], dtype=torch.int64)

        # make target dict
        target = {
            'boxes': boxes,
            'labels': labels,
        }

        # transform tensors, in this case using to tensor
        if self.transform:
            image = self.transform(image)

        return image, target



from torch.utils.data import DataLoader

dataset = ObjectDetectionDataset(dataframe=train_df, img_dir='/content/train/', class_dict=class_dict, transform=transform)
train_loader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=0)
val_loader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=0)
for images, targets in train_loader:
  for boxes, labels in zip(targets['boxes'], targets['labels']):
    print(boxes)
    print(labels)
  break



from torchvision.models.detection import retinanet_resnet50_fpn
import matplotlib.pyplot as plt

# Initialize the model
device1 = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = retinanet_resnet50_fpn(pretrained=True)

# Create optimizer and epochs
optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
epochs = 10

classification_losses = []
bounding_box_losses = []
accuracy_scores = []

for epoch in range(epochs):
    model = model.to(device1)
    running_classification_loss = 0.0
    running_bounding_box_loss = 0.0

    model.train()

    for images, targets in train_loader:

        images = [image.to(device1) for image in images]

        target_dict = []
        for target in targets:
          for boxes, labels in zip(targets['boxes'], targets['labels']):
            boxes = boxes.to(device1)
            labels = labels.to(device1)

            target_dict.append({
                'boxes': boxes,
                'labels': labels
            })

        boxes, labels = zip(targets['boxes'], targets['labels'])

        targets = target_dict
        print(boxes, labels)
        # Making zero gradients
        optimizer.zero_grad()

        # Model forward pass
        loss_dict = model(images, targets)

        # Generate losses
        losses = sum(loss for loss in loss_dict.values())

        # Model backpropagation
        # losses.backward()
        # optimizer.step()

        # print(targets)

        # Accumulate losses for reporting
        running_classification_loss += loss_dict['classification'].item()
        running_bounding_box_loss += loss_dict['bbox_regression'].item()

    print(f"Epoch [{epoch+1}/{epochs}], Classification Loss: {running_classification_loss:.2f}%, Bounding Box Loss: {running_bounding_box_loss:.2f}%")

    model.eval()
    current_correct_predictions = 0
    total_labels = 0

    with torch.no_grad():
        for images, targets in val_loader:
            images = [image.to(device1) for image in images]
            target_dict = []
            for target in targets:
              for boxes, labels in zip(targets['boxes'], targets['labels']):
                boxes = boxes.to(device1)
                labels = labels.to(device1)

                target_dict.append({
                    'boxes': boxes,
                    'labels': labels
                })

            boxes, labels = zip(targets['boxes'], targets['labels'])

            targets = target_dict

            predictions = model(images)
            for target, prediction in zip(targets, predictions):
                predicted_labels = prediction['labels']
                true_labels = target['labels']

                current_correct_predictions += (predicted_labels == true_labels).sum().item()
                total_labels += len(true_labels)

    accuracy = current_correct_predictions / total_labels
    print(f"Validation Accuracy: {accuracy:.2f}%")



# Visualization
plt.figure(figsize=(20, 6))
plt.subplot(1, 2, 1)
plt.plot(classification_losses, label="Classification Loss")
plt.plot(bounding_box_losses, label="Bounding Box Regression Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.title("Loss During Training")

plt.subplot(1, 2, 2)
plt.plot(accuracy_scores, label="Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Accuracy During Training")



# Visualization
plt.figure(figsize=(20, 6))
plt.subplot(1, 2, 1)
plt.plot(classification_losses, label="Classification Loss")
plt.plot(bounding_box_losses, label="Bounding Box Regression Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.title("Loss During Training")

plt.subplot(1, 2, 2)
plt.plot(accuracy_scores, label="Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Accuracy During Training")



import numpy as np
from torchvision.ops import box_iou

def calculate_iou(predicted_boxes, ground_truth_boxes):
    iou = box_iou(predicted_boxes, ground_truth_boxes)
    return iou

def evaluate_predictions(predictions, ground_truth):

    pred_boxes = predictions['boxes']
    gt_boxes = ground_truth['boxes']

    iou = calculate_iou(pred_boxes, gt_boxes)

    return iou

predictions = model(image_tensor)

ground_truth = target

iou_scores = evaluate_predictions(predictions[0], ground_truth)
print(f"IoU scores: {iou_scores}")



import matplotlib.patches as patches
from torchvision.transforms.functional import to_pil_image
from torchvision.ops import box_iou


def calculate_iou(predicted_boxes, ground_truth_boxes):
    iou = box_iou(predicted_boxes, ground_truth_boxes)
    return iou

def evaluate_predictions(predictions, ground_truth):
    pred_boxes = predictions['boxes']
    gt_boxes = ground_truth['boxes']
    iou = calculate_iou(pred_boxes, gt_boxes)
    return iou


def display_predictions_with_iou(images, predictions, ground_truths, iou_scores, class_dict, num_images=10):
    num_images = min(num_images, len(images))
    for i in range(num_images):
        image = images[i].cpu()
        prediction = predictions[i]
        ground_truth = ground_truths[i]

        pil_image = to_pil_image(image)
        fig, ax = plt.subplots(1, 1, figsize=(8, 8))
        ax.imshow(pil_image)

        for bbox, label, score in zip(prediction['boxes'], prediction['labels'], prediction['scores']):
            bbox = bbox.cpu().numpy()
            label = label.item()
            score = score.item()
            rect = patches.Rectangle(
                (bbox[0], bbox[1]), bbox[2] - bbox[0], bbox[3] - bbox[1],
                linewidth=2, edgecolor='r', facecolor='none'
            )
            ax.add_patch(rect)
            class_name = [k for k, v in class_dict.items() if v == label][0]
            ax.text(
                bbox[0], bbox[1] - 5,
                f"{class_name}: {score:.2f}",
                color='red', fontsize=10, backgroundcolor='white'
            )

        
        for gt_bbox, gt_label in zip(ground_truth['boxes'], ground_truth['labels']):
            gt_bbox = gt_bbox.cpu().numpy()
            gt_label = gt_label.item()
            rect = patches.Rectangle(
                (gt_bbox[0], gt_bbox[1]), gt_bbox[2] - gt_bbox[0], gt_bbox[3] - gt_bbox[1],
                linewidth=2, edgecolor='g', facecolor='none', linestyle='dashed'
            )
            ax.add_patch(rect)

        
        ax.text(10, 10, f"IoU: {iou_scores[i].mean().item():.2f}",
                color='blue', fontsize=12, backgroundcolor='white')

        plt.axis('off')
        plt.show()


model.eval()
class_dict = {0: "ClassA", 1: "ClassB", 2: "ClassC"}

with torch.no_grad():
    images, targets = next(iter(test_loader)) 
    images = [image.to(device) for image in images]
    predictions = model(images)

    iou_scores = [evaluate_predictions(pred, target) for pred, target in zip(predictions, targets)]

display_predictions_with_iou(images, predictions, targets, iou_scores, class_dict)

some of this code should work, idk if the output to show image predictions works as intended but here it is i guess
